{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data from : https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying IMDB Sentiments with BERT Tokenizer and Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def clean_text(sentence):\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    sentence = TAG_RE.sub('', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_reviews'] = df['review'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production br br The filmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was wonderful way to spend time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this movie did down right good job I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I am Catholic taught in parochial elementary s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I going to have to disagree with the previous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                           clean_reviews  \n",
       "0      One of the other reviewers has mentioned that ...  \n",
       "1      A wonderful little production br br The filmin...  \n",
       "2      I thought this was wonderful way to spend time...  \n",
       "3      Basically there a family where little boy Jake...  \n",
       "4      Petter Mattei Love in the Time of Money is vis...  \n",
       "...                                                  ...  \n",
       "49995  I thought this movie did down right good job I...  \n",
       "49996  Bad plot bad dialogue bad acting idiotic direc...  \n",
       "49997  I am Catholic taught in parochial elementary s...  \n",
       "49998  I going to have to disagree with the previous ...  \n",
       "49999  No one expects the Star Trek movies to be high...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment']\n",
    "\n",
    "y = df['sentiment'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines br br At first it was very odd and pretty funny but as the movie progressed didn find the jokes or oddness funny anymore br br Its low budget film thats never problem in itself there were some pretty interesting characters but eventually just lost interest br br imagine this film would appeal to stoner who is currently partaking br br For something similar but better try Brother from another planet \n"
     ]
    }
   ],
   "source": [
    "print(df['clean_reviews'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salman', 'brother', 'is', 'an', 'actor', 'for', 'the', 'masses']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Salman Brother is an actor for the masses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2123, 2102, 2022, 2061, 8689, 2389]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"dont be so judgmental\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = [tokenize_reviews(review) for review in df.review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_with_len = [[review, y[i], len(review)]\n",
    "                 for i, review in enumerate(tokenized_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(reviews_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_with_len.sort(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=23054, shape=(32, 28), dtype=int32, numpy=\n",
       " array([[ 3191,  1996,  2338,  1010,  5293,  1996,  3185,   999,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 3078,  5436,   999,  3078,  3257,   999,  3532,  7613,  1012,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2023,  3185,  2003,  6659,  2021,  2009,  2038,  2070,  2204,\n",
       "          3896,  1012,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2054,  1037,  5896,  1010,  2054,  1037,  2466,  1010,  2054,\n",
       "          1037,  6752,   999,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 8235,  1998,  3048,  4616,  2011,  3419,  2457, 27727,  1998,\n",
       "          2848, 16133,  1012,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 1045,  2876,  1005,  1056,  9278,  2023,  2028,  2130,  2006,\n",
       "          7922, 12635,  2305,  1012,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 1045,  3246,  2023,  2177,  1997,  2143,  1011, 11153,  2196,\n",
       "          2128,  1011, 15908,  2015,  1012,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 7918, 14674,  7662,  2003,  6581,  2003,  2023,  2143,  1012,\n",
       "          2002,  3084,  1037, 17160,  2450,  1012,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [11861,  1996, 21442,  6895,  3238,  2515,  1037,  2210, 22759,\n",
       "          6198,  1998,  1037,  3185,  2087, 12487,   999,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2023,  2003,  1037,  2307,  3185,  1012,  2205,  2919,  2009,\n",
       "          2003,  2025,  2800,  2006,  2188,  2678,  1012,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2062, 23873,  3993,  1010,  2062, 11259,  1010,  2172,  1010,\n",
       "          2172,  2062, 14888,  1012,  1012,  1012,  1012,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2017,  1005,  1040,  2488,  5454,  2703,  2310, 25032,  8913,\n",
       "          8159,  1005,  1055,  2130,  2065,  2017,  2031,  3427,  2009,\n",
       "          1012,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2074,  2293,  1996,  6970, 13068,  2090,  2048,  2307,  3494,\n",
       "          1997,  2754,  1004,  3898,  1011,  2310,  3593,  2102,  1004,\n",
       "          6287,  5974,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2023,  2003,  1996, 15764,  3185,  2544,  1997,  8429,  1012,\n",
       "         24905, 17988,  7659,  2498,  1010,  2021,  2045,  2024,  2053,\n",
       "         13842,  5312,  1012,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 1045,  2123,  1005,  1056,  2113,  2339,  1045,  2066,  2023,\n",
       "          3185,  2061,  2092,  1010,  2021,  1045,  2196,  2131,  5458,\n",
       "          1997,  3666,  2009,  1012,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2023,  2003,  1037,  2204,  2143,  1012,  2023,  2003,  2200,\n",
       "          6057,  1012,  2664,  2044,  2023,  2143,  2045,  2020,  2053,\n",
       "          2204,  8471,  3152,   999,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2146,  1010, 11771,  1010,  1038,  8523,  8458,  6633,  3560,\n",
       "          1012,  2196,  2031,  1045,  2042,  2061,  5580,  2000,  2156,\n",
       "          4566,  6495,  4897,  1012,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 1037,  7244,  3185,  1012,  2009,  2003,  2440,  1997,  6699,\n",
       "          1998,  6919,  3772,  1012,  1045,  2071,  2031,  2938,  2083,\n",
       "          2009,  1037,  2117,  2051,  1012,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2005,  5760,  7788,  4393,  8808,  2498,  2064, 12826,  2000,\n",
       "          1996, 11056,  3152,  1012,  1045,  3811, 16755,  2169,  1998,\n",
       "          2296,  2028,  1997,  2068,  1012,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [ 1037,  5790,  1997,  1000,  1015,  1000,  2515,  2025,  4088,\n",
       "          2000,  4671,  2129, 10634,  1010,  2139, 24128,  1998, 21660,\n",
       "          2135,  2919,  2023,  3185,  2003,  1012,     0,     0,     0,\n",
       "             0],\n",
       "        [ 2028,  1997,  1996,  4569, 15580,  2102,  5691,  2081,  1999,\n",
       "          3522,  2086,  1012,  2204, 23191,  1010,  5436,  1998, 11813,\n",
       "          6370,  2191,  2023,  2028,  1037,  4438,     0,     0,     0,\n",
       "             0],\n",
       "        [ 7078, 10392,   999,  3649,  1045,  2360,  2876,  1005,  1056,\n",
       "          2079,  2023,  2104,  9250,  3185,  1996,  3425,  2009, 17210,\n",
       "          1012,  3422,  2009,  2085,   999, 10392,   999,     0,     0,\n",
       "             0],\n",
       "        [ 2053,  7615,  1011,  5236,  3185,  1010,  3772,  2779,  2030,\n",
       "          4788,  1012,  1012,  1012,  9000,  1011,  2053,  3168,  2012,\n",
       "          2035,  1012,  1012,  1012, 13558,  2009,   999,     0,     0,\n",
       "             0],\n",
       "        [ 1037,  2033,  6491, 11124,  6774,  2143,  2008,  5121,  7906,\n",
       "          2115,  3086,  1012,  1012,  1012,  3841, 13196,  2003, 17160,\n",
       "          1006,  1998, 26103,  1007,  2000,  3422,  1012,     0,     0,\n",
       "             0],\n",
       "        [ 2023,  3185,  2097,  2467,  2022,  1037,  5934,  1998,  3185,\n",
       "          4438,  1010,  2004,  2146,  2004,  2045,  2024,  2145,  2111,\n",
       "          2040,  6170,  1010,  3153,  1010,  1998,  2552,  1012,     0,\n",
       "             0],\n",
       "        [ 7615,  2023,  3185,  2003,  5263,  1012,  2003,  6659,  1010,\n",
       "          2200, 17727,  3217,  3676,  3468,  1010,  2919,  7613,  1041,\n",
       "          3257,  1012,  2025,  2298,   999,   999,   999,   999,   999,\n",
       "             0],\n",
       "        [ 5525,  2517,  2005,  1996,  2754,  1012, 12038,  2021,  4276,\n",
       "         19927,  1012,  2129,  2064,  2017,  2175,  3308,  2007,  6798,\n",
       "          9482,  1010, 14439,  1998, 21442,  2571, 15578,  4948,  1012,\n",
       "             0],\n",
       "        [ 2023,  2003,  1996,  4602,  3185,  2412,  1012,  2065,  2017,\n",
       "          2031,  2517,  2009,  2125,  2007,  2041,  2412,  3773,  2009,\n",
       "          1012,  2017,  2442,  2507,  2009,  1037,  2117,  3046,  1012,\n",
       "             0],\n",
       "        [ 2307,  3185,  1011,  2926,  1996,  2189,  1011,  3802,  2696,\n",
       "          2508,  1011,  1000,  2012,  2197,  1000,  1012,  2023,  8847,\n",
       "          6702,  2043,  2017,  2031,  2633,  2179,  2008,  2569,  2619,\n",
       "          1012],\n",
       "        [ 5587,  2023,  2210, 17070,  2000,  2115,  2862,  1997,  6209,\n",
       "         24945,  1012,  2009,  2003,  1026,  7987,  1013,  1028,  1026,\n",
       "          7987,  1013,  1028,  4086,  1010,  6057,  1010,  1998,  2203,\n",
       "         27242],\n",
       "        [ 6283,  2009,  2007,  2035,  2026,  2108,  1012,  5409,  3185,\n",
       "          2412,  1012, 10597,  1011, 21985,  1012,  2393,  2033,  1012,\n",
       "          2009,  2001,  2008,  2919,  1012,  3404,  2033,   999,   999,\n",
       "           999],\n",
       "        [ 1996,  3494,  2024,  4406,  3085,  1998,  1996,  5896,  2003,\n",
       "          9643,  1012,  2009,  1005,  1055,  1037,  5949,  1997,  1996,\n",
       "         11725,  1997,  7939, 13765,  3726,  1998,  8740,  2618, 19231,\n",
       "          1012]])>,\n",
       " <tf.Tensor: id=23055, shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0])>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"nadam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"nadam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 493s 350ms/step - loss: 0.2946 - accuracy: 0.8692\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 496s 352ms/step - loss: 0.1343 - accuracy: 0.9511\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 473s 336ms/step - loss: 0.0521 - accuracy: 0.9824\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 478s 340ms/step - loss: 0.0266 - accuracy: 0.9910\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 475s 338ms/step - loss: 0.0214 - accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2821a558408>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e78d3bed94c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "results = text_model.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
